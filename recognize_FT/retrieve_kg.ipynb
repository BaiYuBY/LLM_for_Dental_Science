{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neo4j\n",
    "import jsonlines\n",
    "import csv\n",
    "import pandas as pd\n",
    "from py2neo import Graph, Node, Relationship\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import os\n",
    "os.chdir(r\"A:\\Desktop\\COMP7600\\dataset_label_fine-tune\\retrieve_part\")\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import faiss\n",
    "from transformers import ( \n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import bitsandbytes\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load():\n",
    "    embedder_model = SentenceTransformer(\"A:\\Desktop\\COMP7600\\dataset_label_fine-tune\\models\\pubmedbert-base-embeddings\")\n",
    "    return embedder_model\n",
    "\n",
    "def get_entity_embeddings(entities):\n",
    "    embeddings = []\n",
    "    for entity in tqdm(entities, desc=\"Processing entity embeddings\"):\n",
    "        output_embedding = embedder_model.encode(entity)\n",
    "        embeddings.append(output_embedding)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def L2_distance_search(data, k=5, threshold=50):\n",
    "    entity_embeddings = get_entity_embeddings(data)\n",
    "\n",
    "    similar_entities = []\n",
    "    for idx, entity_embedding in enumerate(tqdm(entity_embeddings, desc=\"Processing similarity search\")):\n",
    "        query_vector = np.array([entity_embedding], dtype=np.float32)\n",
    "        distances, result_ids = index.search(query_vector, k=k)\n",
    "\n",
    "        for i, dist in enumerate(distances[0]):\n",
    "            if dist <= threshold:\n",
    "\n",
    "                entity_name = data[idx]\n",
    "                similar_entities.append((result_ids[0][i], entity_name, dist))\n",
    "    return similar_entities\n",
    "\n",
    "def normalize_embeddings(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    norms = np.where(norms == 0, 1, norms)  # 避免除以零\n",
    "    return embeddings / norms\n",
    "\n",
    "def cosine_distance_search(data, k=5, threshold=0.6):\n",
    "    entity_embeddings = get_entity_embeddings(data)\n",
    "\n",
    "    similar_entities = []\n",
    "    for idx, entity_embedding in enumerate(tqdm(entity_embeddings, desc=\"Processing similarity search\")):\n",
    "        query_vector = np.array(normalize_embeddings([entity_embedding]), dtype=np.float32)\n",
    "        distances, result_ids = index.search(query_vector, k=k)\n",
    "        \n",
    "        for i, dist in enumerate(distances[0]):\n",
    "            similarity_score = dist\n",
    "            \n",
    "            if similarity_score >= threshold:\n",
    "                entity_name = data[idx]\n",
    "                similar_entities.append((result_ids[0][i], entity_name, similarity_score))\n",
    "                \n",
    "    return similar_entities\n",
    "\n",
    "def read_id_table():\n",
    "    df = pd.read_csv('table.csv')\n",
    "    return df\n",
    "\n",
    "def get_type(name):\n",
    "    query = \"\"\"\n",
    "    MATCH (n)\n",
    "    WHERE n.name IN \"\"\" + str(name) + \"\"\"\n",
    "    RETURN n.name AS name, labels(n) AS types\"\"\"\n",
    "    result = graph.run(query)\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "def get_relation_list():\n",
    "    df = pd.read_csv('./kg1.csv')\n",
    "\n",
    "    type_relation_dict = defaultdict(set)  # 使用 set 来自动去重\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        subject_type = row['subject_type']\n",
    "        relation = row['relation']\n",
    "        type_relation_dict[subject_type].add(relation)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        object_type = row['object_type']\n",
    "        relation = row['relation']\n",
    "        type_relation_dict[object_type].add(relation)\n",
    "\n",
    "    result_dict = {key: len(list(value)) for key, value in type_relation_dict.items()}\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def get_static_list():\n",
    "    df = pd.read_csv('./kg1.csv')\n",
    "\n",
    "    type_relation_dict = defaultdict(set)  # 使用 set 来自动去重\n",
    "    count_entity_table = {}\n",
    "    count_relation_table = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        subject_name = row['subject']\n",
    "        subject_type = row['subject_type']\n",
    "        object_name = row['object']\n",
    "        object_type = row['object_type']\n",
    "        relation = row['relation']\n",
    "\n",
    "        # if subject_type not in count_entity_table.keys():\n",
    "        #     count_entity_table[subject_type] = 0\n",
    "        # else:\n",
    "        #     count_entity_table[subject_type] += 1\n",
    "\n",
    "        # if object_type not in count_entity_table.keys():\n",
    "        #     count_entity_table[object_type] = 0\n",
    "        # else:\n",
    "        #     count_entity_table[object_type] += 1\n",
    "\n",
    "        # if object_type not in count_entity_table.keys():\n",
    "        #     count_entity_table[object_type] = 0\n",
    "        # else:\n",
    "        #     count_entity_table[object_type] += 1\n",
    "\n",
    "        if relation not in count_relation_table.keys():\n",
    "            count_relation_table[relation] = 0\n",
    "        else:\n",
    "            count_relation_table[relation] += 1\n",
    "        type_relation_dict[object_type].add(object_name)\n",
    "        type_relation_dict[subject_type].add(subject_name)\n",
    "\n",
    "\n",
    "    result_dict = {key: len(list(value)) for key, value in type_relation_dict.items()}\n",
    "\n",
    "    return count_entity_table, count_relation_table, result_dict\n",
    "\n",
    "def build_query(entities, relationships):\n",
    "    entity_conditions = []\n",
    "    for entity in entities:\n",
    "        name = entity['name']\n",
    "        types = entity['types']\n",
    "        type_conditions = ''\n",
    "\n",
    "        type_conditions = \" OR \".join([f\"(n:`{t}` AND ({''' OR '''.join([f'r:`{r}`' for r in relationships[t]])}))\" for t in types])\n",
    "        entity_conditions.append(f\"(n.name = '{name}' AND ({type_conditions}))\")\n",
    "\n",
    "    entity_query = \" OR \".join(entity_conditions)\n",
    "\n",
    "    # relationship_types = []\n",
    "    # for typ in relationships:\n",
    "    #     relationship_types.extend(relationships[typ])\n",
    "    # relationship_query = \" OR \".join([f\"r:`{rel}`\" for rel in relationship_types])\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    MATCH (n)-[r]-(m)\n",
    "    WHERE ({entity_query})\n",
    "    RETURN n.name AS entity_name, labels(n) AS entity_type, m.name AS related_entity_name, labels(m) AS related_entity_type, type(r) AS relationship_type\n",
    "    \"\"\"\n",
    "    return query\n",
    "\n",
    "def generate_prompt(data):\n",
    "    SYSTEM_PROMPT = f\"\"\"You are given a list of entities, their types and their relationships.\"\"\"\n",
    "    prompt = ''\n",
    "    for entity in data:\n",
    "        prompt += f\"The entity ({entity['entity_name']}, type: {entity['entity_type'][0]}) and its related entity ({entity['related_entity_name']}, type: {entity['related_entity_type'][0]}) are related by the relationship type: {entity['relationship_type']}.\\n\"\n",
    "    return SYSTEM_PROMPT + prompt.strip('\\n')\n",
    "\n",
    "def get_important_entities(question):\n",
    "    test_text =\"\"\"### Instruction: \n",
    "    Identify the important entities in the question that you need further extral knowledge to answer.\n",
    "\n",
    "    ### System Prompt: \n",
    "    The following is an unstructured question, please extract the important entities in the text, and the output should follow the following format: [entity1, entity2, entity3]. No more note or explain is needed, only output a list.\n",
    "    This extracted entities are used for searching further help and knowledge in an extal database. Thus, only find the entities that you are not sure or not familiar with. If you cannot find any entities that satisfy the above requirements, please output: []. No more note or explan is needed.\n",
    "\n",
    "    ### Input:\n",
    "    \"\"\" + question + \"\"\"\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    device = \"cuda:0\"\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n",
    "    outputs = test_model.generate(**inputs, max_new_tokens=2000)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(response)\n",
    "    return response.split('### Response:')[-1].strip()\n",
    "\n",
    "def cypher_generate(check_name):\n",
    "    data = get_type(check_name)\n",
    "\n",
    "    results = [{\"name\": record[\"name\"], \"type\": record[\"types\"]} for record in data]\n",
    "\n",
    "    aggregated_data = defaultdict(list)\n",
    "    \n",
    "    for record in results:\n",
    "        name = record[\"name\"]\n",
    "        types = record[\"type\"]\n",
    "        aggregated_data[name].extend(types)\n",
    "    \n",
    "    results = [{\"name\": name, \"types\": list(set(types))} for name, types in aggregated_data.items()]\n",
    "    return results\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        'A:\\Desktop\\COMP7600\\dataset_label_fine-tune\\models\\Llama3-Med42-8B',\n",
    "        device_map={\"\":0},\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained('A:\\Desktop\\COMP7600\\dataset_label_fine-tune\\models\\Llama3-Med42-8B')\n",
    "    lora_config = LoraConfig.from_pretrained('./checkpoint-50')\n",
    "    test_model = get_peft_model(model, lora_config)\n",
    "    # tokenizer.pad_token = tokenizer.eos_token\n",
    "    return test_model, tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kg_static.txt\", \"w\") as f:\n",
    "    f.write(str(get_static_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8115"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#L2 distnace\n",
    "df = read_id_table()\n",
    "embedder_model = model_load()\n",
    "\n",
    "loaded_embeddings = np.load('./combined_entity_embeddings.npy')\n",
    "entities_ids = np.array(list(df.to_dict()['name'].keys()))\n",
    "\n",
    "dimension = loaded_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index_with_ids = faiss.IndexIDMap(index)\n",
    "index_with_ids.add_with_ids(loaded_embeddings, entities_ids)\n",
    "\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8115"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "#cosine distnace\n",
    "df = read_id_table()\n",
    "embedder_model = model_load()\n",
    "\n",
    "loaded_embeddings = np.load('./combined_entity_embeddings.npy')\n",
    "normalize_embeddings = normalize_embeddings(loaded_embeddings)\n",
    "entities_ids = np.array(list(df.to_dict()['name'].keys()))\n",
    "\n",
    "dimension = loaded_embeddings.shape[1]\n",
    "print(dimension)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index_with_ids = faiss.IndexIDMap(index)\n",
    "index_with_ids.add_with_ids(normalize_embeddings, entities_ids)\n",
    "\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"neo4j://101.34.58.20:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"Tzmt541881\"\n",
    "\n",
    "graph = Graph(uri, auth=(username, password))\n",
    "result_dict = get_relation_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " distinct_entities_count \n",
      "-------------------------\n",
      "                    5942 \n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "#检验连接是否成功\n",
    "query = \"\"\"\n",
    "MATCH (n)\n",
    "RETURN COUNT(DISTINCT n) AS distinct_entities_count\"\"\"\n",
    "result = graph.run(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'disease': ['parent-child', 'medicine_side-effect_disease', 'prevent', 'is', 'medicine_contraindication_disease', 'disease_treatment', 'disease_caused_disease', 'disease_usually happens_population', 'disease_oral part', 'disease_lack response_examination', 'medicine_treats_disease', 'disease_symptom', 'causes_disease', 'disease_examination', 'disease_has_clinical manifestations', 'disease_has_description'], 'symptom': ['symptom_has_description', 'parent-child', 'prevent', 'is', 'symptom_cause_symptom', 'medicine_side-effect_symptom', 'medicine_reduce_symptom', 'symptom_oral part', 'disease_symptom', 'disease_has_clinical manifestations'], 'clinical manifestations': ['parent-child', 'clinical manifestations_from_examination', 'clinical manifestations_happens at_oral part', 'clinical manifestations_use_treatment', 'is', 'disease_has_clinical manifestations'], 'description': ['symptom_has_description', 'parent-child', 'is', 'examination_has_description', 'medicine_has_description', 'disease_has_description', 'materials_has_description', 'disease_has_clinical manifestations', 'treatment_has_description'], 'treatment': ['parent-child', 'clinical manifestations_use_treatment', 'is', 'disease_treatment', 'treatment_frequency', 'disease_has_clinical manifestations', 'treatment_has_description'], 'instrument': ['parent-child', 'disease_has_clinical manifestations', 'is'], 'examination': ['parent-child', 'clinical manifestations_from_examination', 'is', 'examination_has_description', 'examination_at_oral part', 'disease_lack response_examination', 'disease_examination', 'disease_has_clinical manifestations'], 'population': ['parent-child', 'is', 'disease_usually happens_population', 'medicine_contraindicates_population', 'disease_has_clinical manifestations'], 'oral part': ['parent-child', 'clinical manifestations_happens at_oral part', 'is', 'disease_oral part', 'examination_at_oral part', 'symptom_oral part', 'disease_has_clinical manifestations'], 'materials': ['parent-child', 'materials_has_description', 'is'], 'medicine': ['parent-child', 'medicine_side-effect_disease', 'medicine_contraindicates_population', 'is', 'medicine_contraindication_disease', 'medicine_has_description', 'medicine_reduce_symptom', 'medicine_side-effect_symptom', 'medicine_treats_disease'], 'prevention': ['parent-child', 'disease_has_clinical manifestations', 'is'], 'causes': ['parent-child', 'causes_disease', 'disease_has_clinical manifestations', 'is'], 'usage': ['is'], 'frequency': ['disease_has_clinical manifestations', 'treatment_frequency', 'is']}\n"
     ]
    }
   ],
   "source": [
    "print(get_relation_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing entity embeddings: 100%|██████████| 3/3 [00:00<00:00, 16.94it/s]\n",
      "Processing similarity search: 100%|██████████| 3/3 [00:00<00:00, 750.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " name                  | types         \n",
      "-----------------------|---------------\n",
      " parotid gland         | ['disease']   \n",
      " major salivary glands | ['disease']   \n",
      " salivary glands       | ['treatment'] \n",
      "\n",
      " name                  | types         \n",
      "-----------------------|---------------\n",
      " parotid gland         | ['disease']   \n",
      " major salivary glands | ['disease']   \n",
      " salivary glands       | ['treatment'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test_model, tokenizer = load_model_and_tokenizer()\n",
    "# question = 'A biopsy specimen of the lower lip salivary glands showed replacement of parenchymal tissue by lymphocytes. The patient also had xerostomia and eratoconjunctivitis sicca. These findings are indicative of which of the following?'\n",
    "# input_entity = get_important_entities(question)\n",
    "\n",
    "input_entity = ['lower lip salivary glands', 'xerostomia', 'eratoconjunctivitis sicca']\n",
    "results = cosine_distance_search(input_entity, k=10)\n",
    "check_name = []\n",
    "\n",
    "for i in results:\n",
    "    check_name.append(df.to_dict()['name'][i[0]])\n",
    "\n",
    "res = cypher_generate(check_name)\n",
    "\n",
    "query = build_query(cypher_generate(check_name), result_dict)\n",
    "query_results = graph.run(query).data()\n",
    "\n",
    "final_prompt = generate_prompt(query_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prompt.txt', 'w') as file:\n",
    "    file.write(final_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
