{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#直接读取PDF文件内容，并保存到txt文件中。我们不用\n",
    "\n",
    "from io import StringIO\n",
    "from io import open\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfpage import PDFPage\n",
    " \n",
    "def read_pdf(pdf):\n",
    "    # resource manager\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    # device\n",
    "    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "    PDFPage.get_pages(rsrcmgr, device, pdf)\n",
    "    device.close()\n",
    "    content = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    # 获取所有行\n",
    "    lines = str(content).split(\"\\n\")\n",
    "    return lines\n",
    " \n",
    "with open('./temp/temp.pdf', \"rb\") as my_pdf:\n",
    "    with open('result.txt','w+') as f:\n",
    "        for x in read_pdf(my_pdf):\n",
    "            f.write(x)\n",
    "            print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pages: 197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#直接读取pdf文件内容，并打印出来。我们不用\n",
    "\n",
    "import PyPDF2\n",
    "\n",
    "def Read(startPage, endPage):\n",
    "    text = []\n",
    "    cleanText = \"\"\n",
    "    try:\n",
    "        pdfFileObj = open('./temp/temp.pdf', 'rb')\n",
    "        pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "        num_pages = len(pdfReader.pages)\n",
    "        print(f\"Total number of pages: {num_pages}\")\n",
    "        \n",
    "        if endPage >= num_pages:\n",
    "            print(f\"End page {endPage} is out of range. Total pages are {num_pages}.\")\n",
    "            return\n",
    "        \n",
    "        while startPage <= endPage:\n",
    "            pageObj = pdfReader.pages[startPage]\n",
    "            page_text = pageObj.extract_text()\n",
    "            if page_text:\n",
    "                text.append(page_text)\n",
    "            startPage += 1\n",
    "        \n",
    "        pdfFileObj.close()\n",
    "        \n",
    "        for page_text in text:\n",
    "            cleanText += page_text + \"\\n\"\n",
    "        \n",
    "        cleanText = cleanText.strip()\n",
    "        print(cleanText)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Program interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "Read(25, 35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这是写的切割的临时代码块。不用看。\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from PyPDF2 import PdfWriter, PdfReader\n",
    "import glob\n",
    "\n",
    "PDF_file_path = r'./Part3/p3_3.pdf'\n",
    "output_file_path = r'./temp'\n",
    "max_pages = 750\n",
    "# doc = fitz.open(PDF_file_path)\n",
    "\n",
    "def clear_pdf_files(directory):\n",
    "    # 获取目录下所有PDF文件的路径\n",
    "    if os.path.exists(directory) == False:\n",
    "        os.mkdir(directory)\n",
    "    pdf_files = glob.glob(os.path.join(directory, '*.pdf'))\n",
    "    \n",
    "    # 删除每个PDF文件\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            os.remove(pdf_file)\n",
    "            print(f\"Deleted {pdf_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {pdf_file}: {e}\")\n",
    "\n",
    "clear_pdf_files(output_file_path)\n",
    "\n",
    "def split_pdf(input_path, max_pages, output_prefix):\n",
    "    # 打开PDF文件\n",
    "    pdf_document = fitz.open(input_path)\n",
    "    total_pages = len(pdf_document)\n",
    "\n",
    "    if total_pages <= max_pages:\n",
    "        print(f\"PDF has {total_pages} pages, no need to split.\")\n",
    "        output_path = os.path.join(output_prefix, \"temp.pdf\")\n",
    "        output_pdf = fitz.open()\n",
    "        output_pdf.insert_pdf(pdf_document, from_page=0, to_page=total_pages)\n",
    "        output_pdf.save(output_path)\n",
    "        output_pdf.close()\n",
    "        return\n",
    "\n",
    "    # 计算需要分割的份数\n",
    "    num_parts = (total_pages + max_pages - 1) // max_pages\n",
    "\n",
    "    for part in range(num_parts):\n",
    "        output_path = os.path.join(output_prefix, f\"temp_part_{part + 1}.pdf\")\n",
    "        output_pdf = fitz.open()\n",
    "\n",
    "        start_page = part * max_pages\n",
    "        end_page = min(start_page + max_pages, total_pages)\n",
    "\n",
    "        for page_num in range(start_page, end_page):\n",
    "            output_pdf.insert_pdf(pdf_document, from_page=page_num, to_page=page_num)\n",
    "\n",
    "        output_pdf.save(output_path)\n",
    "        output_pdf.close()\n",
    "\n",
    "        print(f\"Created {output_path} with pages {start_page + 1} to {end_page}\")\n",
    "\n",
    "split_pdf(PDF_file_path, max_pages, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/11/09 00:53:43] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, ir_optim=True, use_tensorrt=True, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\28407/.paddleocr/whl\\\\det\\\\ch\\\\ch_PP-OCRv4_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\28407/.paddleocr/whl\\\\rec\\\\ch\\\\ch_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='b:\\\\Conda\\\\envs\\\\basic2\\\\lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\ppocr_keys_v1.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='C:\\\\Users\\\\28407/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=True, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='ch', det=False, rec=True, type='ocr', ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e7c91e616c439285613c84d9138be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='显示下一张图片', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee921f3b1214e1aa0068a6476574510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#查看ORC识别时是否有效去除页眉页脚。每一个新的PDF都用这个来看裁剪的参数\n",
    "\n",
    "import fitz\n",
    "import time\n",
    "import os\n",
    "from paddleocr import PaddleOCR\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "ocr = PaddleOCR(det=False, use_gpu=False, enable_mkldnn=True, use_tensorrt=True, use_angle_cls=True, lang='ch')\n",
    "\n",
    "def pdf_to_jpg(name):\n",
    "    pdfdoc = fitz.open(name)\n",
    "    temp = 0\n",
    "    page_index = 0\n",
    "    \n",
    "    def display_next_image(b):\n",
    "        nonlocal page_index\n",
    "        if page_index < pdfdoc.page_count:\n",
    "            page = pdfdoc[page_index]\n",
    "            rotate = int(0)\n",
    "            rect = page.rect\n",
    "\n",
    "            #裁剪范围\n",
    "            crop_top = 50\n",
    "            crop_bottom = 30\n",
    "            crop_right = 40\n",
    "            crop_left = 40\n",
    "\n",
    "            #缩放比例\n",
    "            zoom_x = 0.8\n",
    "            zoom_y = 0.8\n",
    "            rotate = int(0)\n",
    "            trans = fitz.Matrix(zoom_x, zoom_y).prerotate(rotate)\n",
    "            crop = fitz.Rect(rect.x0 + crop_left, rect.y0 + crop_top, rect.x1 - crop_right, rect.y1 - crop_bottom)\n",
    "            trans = fitz.Matrix(zoom_x, zoom_y).prerotate(rotate)\n",
    "            pm_cropped = page.get_pixmap(matrix=trans, alpha=False, clip=crop)\n",
    "            pm_original = page.get_pixmap(matrix=trans, alpha=False)\n",
    "            \n",
    "            # 将 pixmap 转换为图像并显示\n",
    "            img_cropped = widgets.Image(value=pm_cropped.tobytes(), format='png')\n",
    "            img_original = widgets.Image(value=pm_original.tobytes(), format='png')\n",
    "            \n",
    "            output.clear_output(wait=True)\n",
    "            with output:\n",
    "                display(widgets.HBox([img_original, img_cropped]))\n",
    "            page_index += 1\n",
    "        else:\n",
    "            output.clear_output(wait=True)\n",
    "            with output:\n",
    "                print(\"所有图片已显示完毕。\")\n",
    "    \n",
    "    button = widgets.Button(description=\"显示下一张图片\")\n",
    "    output = widgets.Output()\n",
    "    button.on_click(display_next_image)\n",
    "    \n",
    "    display(button, output)\n",
    "\n",
    "pdf_to_jpg(r'./Part3/P3_4.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用OCR技术，将PDF文件转化为文本文件，并且消除页眉页脚内容，现在暂定最佳的PDF读取方法。但我们不用\n",
    "\n",
    "import fitz\n",
    "pdf_file = \"./test4.pdf\"\n",
    "pdf_document = fitz.open(pdf_file)\n",
    "text = \"\"\n",
    "for page_number in range(len(pdf_document)):\n",
    "    page = pdf_document.load_page(page_number)\n",
    "    rect = page.rect\n",
    "\n",
    "    #需要从上一个代码块得到参数\n",
    "    clip_x = 40\n",
    "    clip_y = 85\n",
    "\n",
    "    zoom_x = 16.0\n",
    "    zoom_y =16.0\n",
    "    rotate = int(0)\n",
    "    trans = fitz.Matrix(zoom_x, zoom_y).prerotate(rotate)\n",
    "    crop = fitz.Rect(clip_x, clip_y, rect.width-clip_x, rect.height-clip_y)\n",
    "    # pm = page.get_pixmap(matrix=trans, alpha=False, clip = crop)\n",
    "    # pm._writeIMG('temp.jpg', 1, jpg_quality = 95)\n",
    "    # input()\n",
    "    for block in page.get_text(\"blocks\", clip = crop):\n",
    "        x0, y0, x1, y1 = block[0:4]\n",
    "        text_block = block[4]\n",
    "        # 根据文本块属性过滤表格中的文本\n",
    "        # 这只是一个示例，你可以根据文本块的位置和其他属性来进一步过滤\n",
    "        if y1 - y0 < 20:  # 通过高度过滤小文本块\n",
    "            continue\n",
    "        if \"image\" in text_block:\n",
    "            continue\n",
    "        text += text_block\n",
    "with open(pdf_file.split('.pdf')[0] + '.txt', 'w+') as w:\n",
    "    w.write(text)\n",
    "pdf_document.close()\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytesseract使用OCR图像识别来转换PDF文件中的文字内容。我们不用\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "from io import BytesIO\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'D:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'  # 根据你的安装路径调整\n",
    "testdata_dir_config = '--tessdata-dir \"D:\\\\Program Files\\\\Tesseract-OCR\\\\tessdata\"'\n",
    "\n",
    "if os.path.exists(\"result.txt\"):\n",
    "    os.remove(\"result.txt\")\n",
    "fo = open(\"result.txt\", \"w\")\n",
    "\n",
    "pages = convert_from_path(\"./problem/p1_4.pdf\")\n",
    "\n",
    "# 定义裁剪区域，例如去除页眉和页脚各50像素\n",
    "crop_top = 110\n",
    "crop_bottom = 60\n",
    "crop_right = 40\n",
    "crop_left = 40\n",
    "\n",
    "for i, page in enumerate(pages):\n",
    "    buf = BytesIO()\n",
    "    page.save(buf, format=\"JPEG\")\n",
    "    buf.seek(0)\n",
    "    img_page = Image.open(buf)\n",
    "    \n",
    "    # 获取图像的宽度和高度\n",
    "    width, height = img_page.size\n",
    "    \n",
    "    # 裁剪图像，去除页眉和页脚\n",
    "    img_cropped = img_page.crop((crop_left, crop_top, width - crop_right, height - crop_bottom))\n",
    "    \n",
    "    txt = pytesseract.image_to_string(img_cropped, lang='chi_sim')\n",
    "    fo.write(txt + \"\\n\")\n",
    "    print(f\"No {i} page result: {txt}\")\n",
    "\n",
    "fo.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 查找所有匹配的位置\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m matches \u001b[38;5;241m=\u001b[39m [(match\u001b[38;5;241m.\u001b[39mstart(), match\u001b[38;5;241m.\u001b[39mend()) \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfinditer(pattern, \u001b[43mtext\u001b[49m)]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_file\u001b[39m(text, start, end):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./text3_modify.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# 针对问答式文本的文本格式调整\n",
    "import re\n",
    "\n",
    "pattern = r'\\n\\s{2}\\d+\\.\\s{1}'\n",
    "\n",
    "# 查找所有匹配的位置\n",
    "matches = [(match.start(), match.end()) for match in re.finditer(pattern, text)]\n",
    "\n",
    "def write_file(text, start, end):\n",
    "    with open('./text3_modify.txt', 'a') as f:\n",
    "        f.write(text[start:end] + '\\r\\n')\n",
    "\n",
    "# 打印结果\n",
    "for i in range(len(matches) - 1):\n",
    "    start, end = matches[i]\n",
    "    next_start, next_end = matches[i + 1]\n",
    "    \n",
    "    # 找到下一个换行符的位置\n",
    "    next_newline = end\n",
    "    while True:\n",
    "        next_newline = text.find('\\n', next_newline)\n",
    "        if next_newline == -1:\n",
    "            break\n",
    "        # 检查换行符前是否有问号或句号\n",
    "        if text[next_newline - 2] in '?.':\n",
    "            break\n",
    "        next_newline += 1\n",
    "\n",
    "    if next_newline != -1:\n",
    "        print(f\"位置: {start} 到 {next_newline}, 内容: {text[start:next_newline]}\")\n",
    "        write_file(text, start, next_newline)\n",
    "    else:\n",
    "        print(f\"位置: {start} 到 {end}, 内容: {text[start:end]}\")\n",
    "        write_file(text, start, end)\n",
    "    \n",
    "    # 打印两个问题之间的回答\n",
    "    if next_start > next_newline:\n",
    "        print(f\"位置: {next_newline} 到 {next_start}, 内容: {text[next_newline:next_start]}\")\n",
    "        write_file(text, next_newline, next_start)\n",
    "\n",
    "# 处理最后一个匹配\n",
    "start, end = matches[-1]\n",
    "next_newline = end\n",
    "while True:\n",
    "    next_newline = text.find('\\n', next_newline)\n",
    "    if next_newline == -1:\n",
    "        break\n",
    "    # 检查换行符前是否有问号或句号\n",
    "    if text[next_newline - 2] in '?.':\n",
    "        break\n",
    "    next_newline += 1\n",
    "\n",
    "if next_newline != -1:\n",
    "    print(f\"位置: {start} 到 {next_newline}, 内容: {text[start:next_newline]}\")\n",
    "    write_file(text, start, next_newline)\n",
    "else:\n",
    "    print(f\"位置: {start} 到 {end}, 内容: {text[start:end]}\")\n",
    "    write_file(text, start, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用LlamaParse库进行pdf转txt,每个账号每天只能跑1000页。\n",
    "\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import nest_asyncio\n",
    "# from llama_parse import LlamaParse\n",
    "from PyPDF2 import PdfWriter, PdfReader\n",
    "import glob\n",
    "\n",
    "# 定义裁剪区域，例如去除页眉和页脚各50像素。数值可以是负数。只要能跑就行\n",
    "\n",
    "crop_top = 50\n",
    "crop_bottom = 30\n",
    "crop_right = 40\n",
    "crop_left = 40\n",
    "shift_y = 20\n",
    "shift_x = 50\n",
    "\n",
    "# 这个是提高清晰度的。一般不要小于1。如果pdf字体过小可以适度同比增大x和y的缩放值。\n",
    "zoom_x = 2.0\n",
    "zoom_y = 2.0\n",
    "rotate = int(0)\n",
    "\n",
    "PDF_file_path = r'./Part3/P3_13.pdf'\n",
    "#所有的中间产物文件存放的位置\n",
    "output_file_path = r'./temp'\n",
    "output_txt_path = r'./chunks'\n",
    "max_pages = 750\n",
    "# doc = fitz.open(PDF_file_path)\n",
    "\n",
    "#现在我们所有的临时文件都存在./temp目录下，这个是用来每次清理的。\n",
    "def clear_pdf_files(directory):\n",
    "    # 获取目录下所有PDF文件的路径\n",
    "    if os.path.exists(directory) == False:\n",
    "        os.mkdir(directory)\n",
    "    pdf_files = glob.glob(os.path.join(directory, '*.pdf'))\n",
    "    \n",
    "    # 删除每个PDF文件\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            os.remove(pdf_file)\n",
    "            print(f\"Deleted {pdf_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {pdf_file}: {e}\")\n",
    "\n",
    "clear_pdf_files(output_file_path)\n",
    "\n",
    "#剪切PDF文件，一份最大750页。分割后不推荐一次传输。因为一个api的上限就是1k。可以分次上传后再手动合并。\n",
    "def split_pdf(input_path, max_pages, output_prefix):\n",
    "\n",
    "    pdf_document = fitz.open(input_path)\n",
    "    total_pages = len(pdf_document)\n",
    "\n",
    "    if total_pages <= max_pages:\n",
    "        print(f\"PDF has {total_pages} pages, no need to split.\")\n",
    "        output_path = os.path.join(output_prefix, \"temp.pdf\")\n",
    "        output_pdf = fitz.open()\n",
    "        output_pdf.insert_pdf(pdf_document, from_page=0, to_page=total_pages)\n",
    "        output_pdf.save(output_path)\n",
    "        output_pdf.close()\n",
    "        return\n",
    "\n",
    "    # 计算需要分割的份数\n",
    "    num_parts = (total_pages + max_pages - 1) // max_pages\n",
    "\n",
    "    for part in range(num_parts):\n",
    "        output_path = os.path.join(output_prefix, f\"temp_part_{part + 1}.pdf\")\n",
    "        output_pdf = fitz.open()\n",
    "\n",
    "        start_page = part * max_pages\n",
    "        end_page = min(start_page + max_pages, total_pages)\n",
    "\n",
    "        for page_num in range(start_page, end_page):\n",
    "            output_pdf.insert_pdf(pdf_document, from_page=page_num, to_page=page_num)\n",
    "\n",
    "        output_pdf.save(output_path)\n",
    "        output_pdf.close()\n",
    "\n",
    "        print(f\"Created {output_path} with pages {start_page + 1} to {end_page}\")\n",
    "\n",
    "#新的剪切pdf的方法。如果前面的无法剪切报错cropbox不在mediabox内时可以改用这个。这个产出的pdf文件大小特别大。现在最好先不要用了。。。\n",
    "def crop_pdf_new(input_pdf, output_pdf, crop_left, crop_top, crop_right, crop_bottom):\n",
    "\n",
    "    doc = fitz.open(input_pdf)\n",
    "    \n",
    "    new_doc = fitz.open()\n",
    "\n",
    "    #这个值不要太大，最多就到2吧。因为这个方法是直接保存图片的，每个pdf都特别大，zoom过大pdf会过大，上传会出现问题。\n",
    "    zoom_x = 2.0  # 你可以根据需要调整这个值\n",
    "    zoom_y = 2.0  # 你可以根据需要调整这个值\n",
    "    matrix = fitz.Matrix(zoom_x, zoom_y)\n",
    "\n",
    "    for page in doc:\n",
    "        rect = page.rect\n",
    "        \n",
    "\n",
    "        clip = fitz.Rect(\n",
    "            rect.x0 + crop_left,\n",
    "            rect.y0 + crop_top,\n",
    "            rect.x1 - crop_right,\n",
    "            rect.y1 - crop_bottom\n",
    "        )\n",
    "        \n",
    "        pix = page.get_pixmap(matrix = matrix, clip=clip)\n",
    "        pix.set_dpi(150, 150)  # 设置DPI，降低分辨率\n",
    "        \n",
    "        new_page = new_doc.new_page(width=pix.width, height=pix.height)\n",
    "        \n",
    "        new_page.insert_image(new_page.rect, pixmap=pix)\n",
    "    \n",
    "    new_doc.save(output_pdf)\n",
    "\n",
    "#另一个新的剪切pdf的方法。和旧方法类似。\n",
    "def crop_pdf_pypdf2(input_pdf, output_pdf, crop_left, crop_top, crop_right, crop_bottom, shift_x, shift_y):\n",
    "    with open(input_pdf, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        writer = PdfWriter()\n",
    "\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            page.mediabox.lower_left  = (crop_left + shift_x, crop_bottom + shift_y)\n",
    "            page.mediabox.lower_right  = (page.mediabox.width - crop_right + shift_x, crop_bottom + shift_y)\n",
    "            page.mediabox.upper_left = (crop_left + shift_x, page.mediabox.height - crop_top + shift_y)\n",
    "            page.mediabox.upper_Right = (page.mediabox.width - crop_right + shift_x, page.mediabox.height - crop_top + shift_y)\n",
    "            writer.add_page(page)\n",
    "\n",
    "        with open(output_pdf, 'wb') as output_file:\n",
    "            writer.write(output_file)\n",
    "\n",
    "#旧的剪切pdf的方法。\n",
    "def crop_pdf(input_pdf, output_pdf, crop_left, crop_top, crop_right, crop_bottom):\n",
    "    doc = fitz.open(input_pdf)\n",
    "    for n, page in enumerate(doc):\n",
    "        rect = page.rect\n",
    "        media_box = page.mediabox\n",
    "        # clip.x0 = max(clip.x0, media_box.x0)\n",
    "        # clip.y0 = max(clip.y0, media_box.y0)\n",
    "        # clip.x1 = min(clip.x1, media_box.x1)\n",
    "        # clip.y1 = min(clip.y1, media_box.y1)\n",
    "\n",
    "        # print(rect.x0, rect.y0, rect.x1, rect.y1)\n",
    "        # print(media_box.x0, media_box.y0, media_box.x1, media_box.y1)\n",
    "        if rect.x1 - rect.x0 < 10 or rect.y1 - rect.y0 < 10:\n",
    "            print(f\"Cannot read page {n+1}! Check it manually.\")\n",
    "            continue\n",
    "        trans = fitz.Matrix(zoom_x, zoom_y).prerotate(rotate)\n",
    "        clip = fitz.Rect(max(rect.x0 + crop_left, rect.x0), \\\n",
    "            max(rect.y0 + crop_top, rect.y0), \\\n",
    "                min(rect.x1 - crop_right, rect.x1), \\\n",
    "                    min(rect.y1 - crop_bottom, rect.y1))\n",
    "\n",
    "        # print(clip.x0, clip.y0, clip.x1, clip.y1)\n",
    "\n",
    "        page.set_cropbox(clip)\n",
    "\n",
    "        doc.save(output_pdf)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# API_KEY = 'llx-YWsF0qbwjBsQlhd5RVPqjivhSiRR69FZKEpg36sbxQnGU9vF'\n",
    "# reference: https://cloud.llamaindex.ai/parse\n",
    "\n",
    "# parser = LlamaParse(\n",
    "#     api_key=API_KEY,  # can also be set in your env as LLAMA_CLOUD_API_KEY\n",
    "#     result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
    "#     num_workers=4,  # if multiple files passed, split in `num_workers` API calls\n",
    "#     verbose=True,\n",
    "#     language=\"en\",  # Optionally you can define a language, default=en\n",
    "# )\n",
    "\n",
    "temp_pdf_path = os.path.join(os.path.join(output_file_path, \"temp.pdf\"))\n",
    "\n",
    "# cropped_pdf_doc = crop_pdf(PDF_file_path, temp_pdf_path, crop_left, crop_top, crop_right, crop_bottom)\n",
    "# crop_pdf_pypdf2(PDF_file_path, temp_pdf_path, crop_left, crop_top, crop_right, crop_bottom, shift_x, shift_y)\n",
    "crop_pdf_new(PDF_file_path, temp_pdf_path, crop_left, crop_top, crop_right, crop_bottom)\n",
    "\n",
    "# split_pdf(temp_pdf_path, max_pages, output_file_path)\n",
    "\n",
    "# documents = parser.load_data(temp_pdf_path)\n",
    "\n",
    "# # 处理返回的文档\n",
    "# with open(os.path.join(output_txt_path, PDF_file_path.split('.pdf')[0].split('/')[-1] + '_cropped.txt'), 'w+') as w:\n",
    "    # for i in documents:\n",
    "        # w.write(i.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 08f038e5-24de-4503-9e84-467f593495e1\n",
      "..................."
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "API_KEY = 'llx-diPwN1V1iqNH3eHS7IbDDZmNHLYYjbp067PB0K0mtNKEv2os'\n",
    "# reference: https://cloud.llamaindex.ai/parse\n",
    "\n",
    "pdf_file = './temp/temp.pdf'\n",
    "\n",
    "parser = LlamaParse(\n",
    "    api_key=API_KEY,  # can also be set in your env as LLAMA_CLOUD_API_KEY\n",
    "    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
    "    num_workers=4,  # if multiple files passed, split in `num_workers` API calls\n",
    "    verbose=True,\n",
    "    language=\"en\",  # Optionally you can define a language, default=en\n",
    ")\n",
    "documents = parser.load_data(pdf_file)\n",
    "\n",
    "#处理返回的文档\n",
    "with open('./chunks/P3_13_cropped.txt', 'w+') as w:\n",
    "    for i in documents:\n",
    "        w.write(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file_name = './test3.txt'\n",
    "txt_content = ''\n",
    "with open(txt_file_name, 'r') as f:\n",
    "    for line in f:\n",
    "        txt_content += line.replace('\\n', '')\n",
    "print(txt_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Splitting Text into Sentences\n",
    "def split_text_into_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "sentences = split_text_into_sentences(txt_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = './text3_chunks.txt'\n",
    "\n",
    "with open(output_file, 'w') as o:\n",
    "    for sentence in final_texts:\n",
    "        o.write(sentence + '\\r\\n')\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kmeans结合Spacy提取文本特征并进行文本聚类切chunks，现在采用的切chunk的方法\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "min_token_length = 150\n",
    "max_token_length = 400\n",
    "\n",
    "\n",
    "# Load the Spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def process(text):\n",
    "    doc = nlp(text)\n",
    "    sents = list(doc.sents)\n",
    "    vecs = np.stack([sent.vector / sent.vector_norm for sent in sents])\n",
    "\n",
    "    return sents, vecs\n",
    "\n",
    "def cluster_text(sents, vecs, threshold):\n",
    "    clusters = [[0]]\n",
    "    for i in range(1, len(sents)):\n",
    "        if np.dot(vecs[i], vecs[i-1]) < threshold:\n",
    "            clusters.append([])\n",
    "        clusters[-1].append(i)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def clean_text(text):\n",
    "    # Add your text cleaning process here\n",
    "    return text\n",
    "\n",
    "# Initialize the clusters lengths list and final texts list\n",
    "clusters_lens = []\n",
    "final_texts = []\n",
    "\n",
    "# Process the chunk\n",
    "threshold = 0.3\n",
    "sents, vecs = process(txt_content)\n",
    "\n",
    "# Cluster the sentences\n",
    "clusters = cluster_text(sents, vecs, threshold)\n",
    "\n",
    "for cluster in clusters:\n",
    "    cluster_txt = clean_text(' '.join([sents[i].text for i in cluster]))\n",
    "    cluster_len = len(cluster_txt)\n",
    "    \n",
    "    # Check if the cluster is too short\n",
    "    if cluster_len < min_token_length:\n",
    "        continue\n",
    "    \n",
    "    # Check if the cluster is too long\n",
    "    elif cluster_len > max_token_length:\n",
    "        threshold = 0.5\n",
    "        sents_div, vecs_div = process(cluster_txt)\n",
    "        reclusters = cluster_text(sents_div, vecs_div, threshold)\n",
    "        \n",
    "        for subcluster in reclusters:\n",
    "            div_txt = clean_text(' '.join([sents_div[i].text for i in subcluster]))\n",
    "            div_len = len(div_txt)\n",
    "            \n",
    "            if div_len < min_token_length or div_len > max_token_length:\n",
    "                continue\n",
    "            \n",
    "            clusters_lens.append(div_len)\n",
    "            final_texts.append(div_txt)\n",
    "            \n",
    "    else:\n",
    "        clusters_lens.append(cluster_len)\n",
    "        final_texts.append(cluster_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "文件夹已存在，不必重新创建！\n",
      "文件名:./test6.pdf, 页数: 1, 对象: 59\n",
      "ok,解析pdf结束!\n",
      "总共耗时：0.1509995460510254s\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "#读取PDF中的图片内容(想要连同图片的介绍文字一同提取下来，还没有实现)\n",
    "\n",
    "import fitz\n",
    "import time,os.path,re\n",
    "time1=time.time()\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LAParams,LTTextBoxHorizontal\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed,PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument \n",
    "\n",
    "\n",
    "class CPdf2TxtManager():\n",
    "    # 获取文本内容\n",
    "    def changePdfToText(self, filePath):\n",
    "        # 以二进制读模式打开\n",
    "        file = open(path, 'rb')\n",
    "        # 用文件对象来创建一个pdf文档分析器\n",
    "        parser = PDFParser(file)\n",
    "        # 创建一个PDF文档对象存储文档结构,提供密码初始化,没有就不用传该参数\n",
    "        doc = PDFDocument(parser, password='')\n",
    "        # 检查文件是否允许文本提取\n",
    "        if not doc.is_extractable:\n",
    "            raise PDFTextExtractionNotAllowed\n",
    "        # 创建PDf资源管理器来管理共享资源，#caching = False不缓存\n",
    "        rsrcmgr = PDFResourceManager(caching = False)\n",
    "        # 创建一个PDF设备对象\n",
    "        laparams = LAParams()\n",
    "        # 创建一个PDF页面聚合对象\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        # 创建一个PDF解析器对象\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        # 获得文档的目录(纲要),文档没有纲要会报错\n",
    "        # PDF文档没有目录时会报：raise PDFNoOutlines pdfminer.pdfdocument.PDFNoOutlines\n",
    "        # print(doc.get_outlines())\n",
    "        # 获取page列表\n",
    "        # print(PDFPage.get_pages(doc))\n",
    "        # 循环遍历列表，每次处理一个page的内容\n",
    "        _data =\"\"\n",
    "        for page in PDFPage.create_pages(doc):\n",
    "            interpreter.process_page(page)\n",
    "        # 接受该页面的LTPage对象\n",
    "        layout = device.get_result()\n",
    "        # 这里layout是一个LTPage对象 里面存放着 这个page解析出的各种对象\n",
    "        # 一般包括LTTextBox, LTFigure, LTImage, LTTextBoxHorizontal等等\n",
    "        for x in layout:\n",
    "            if hasattr(x, \"get_text\"):\n",
    "                fileNames = os.path.splitext(filePath)\n",
    "                with open(fileNames[0] + '.txt','a+') as f:\n",
    "                    results = x.get_text()\n",
    "                    f.write(results + '\\n')\n",
    "                    _data+=results\n",
    "        # # 如果x是水平文本对象的话\n",
    "        # if (isinstance(x, LTTextBoxHorizontal)):\n",
    "        # # print('kkk')\n",
    "        # text = re.sub(\" \", '', x.get_text())\n",
    "        # if len(text) != 0:\n",
    "        # _data+=text\n",
    "        return _data\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 获取图片\n",
    "    def pdf2pic(self, path, pic_path):\n",
    "        t0 = time.perf_counter() # 生成图片初始时间\n",
    "        checkXO = r\"/Type(?= */XObject)\" # 使用正则表达式来查找图片\n",
    "        checkIM = r\"/Subtype(?= */Image)\"\n",
    "        doc = fitz.open(path) # 打开pdf文件\n",
    "        imgcount = 0 # 图片计数\n",
    "        lenXREF = doc.xref_length() # 获取对象数量长度\n",
    "        # 打印PDF的信息\n",
    "        print(\"文件名:{}, 页数: {}, 对象: {}\".format(path, len(doc), lenXREF - 1))\n",
    "        # 遍历每一个对象\n",
    "        for i in range(1, lenXREF):\n",
    "            text = doc.xref_object(i) # 定义对象字符串\n",
    "            isXObject = re.search(checkXO, text) # 使用正则表达式查看是否是对象\n",
    "            isImage = re.search(checkIM, text) # 使用正则表达式查看是否是图片\n",
    "            if not isXObject or not isImage: # 如果不是对象也不是图片，则continue\n",
    "                continue\n",
    "            imgcount += 1\n",
    "            pix = fitz.Pixmap(doc, i) # 生成图像对象\n",
    "            new_name = \"图片{}.png\".format(imgcount) # 生成图片的名称\n",
    "            if pix.n < 5: # 如果pix.n<5,可以直接存为PNG\n",
    "                pix.save(os.path.join(pic_path, new_name))\n",
    "            else: # 否则先转换CMYK\n",
    "                pix0 = fitz.Pixmap(fitz.csRGB, pix)\n",
    "                pix0.save(os.path.join(pic_path, new_name))\n",
    "                pix0 = None\n",
    "                pix = None # 释放资源\n",
    "                t1 = time.clock() # 图片完成时间\n",
    "                print(\"运行时间:{}s\".format(t1 - t0))\n",
    "                print(\"提取了{}张图片\".format(imgcount))\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    path = r'./test6.pdf'\n",
    "    pdf2TxtManager = CPdf2TxtManager()\n",
    "    df = pdf2TxtManager.changePdfToText(path)\n",
    "    print(df)\n",
    "    if not df:\n",
    "        pic_path = r'./pictures'\n",
    "    # 创建保存图片的文件夹\n",
    "    if os.path.exists(pic_path):\n",
    "        print(\"文件夹已存在，不必重新创建！\")\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(pic_path)\n",
    "    df = pdf2TxtManager.pdf2pic(path,pic_path)\n",
    "    time2 = time.time()\n",
    "    print('ok,解析pdf结束!')\n",
    "    print('总共耗时：' + str(time2 - time1) + 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Some text here.\n",
      "More text here.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "table_pattern = r'\\b(# Table of Contents|# Contents)\\b'\n",
    "figure_pattern = r'\\b(Figures\\s{1}\\d|Figure|#fi)\\b'\n",
    "\n",
    "def get_pages(check_pattern, text):\n",
    "    pages = text.split('---\\n')\n",
    "\n",
    "    # First check the table of contents\n",
    "    match = re.search(check_pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        matched_pages = []\n",
    "        for n, page in enumerate(pages):\n",
    "            if re.search(check_pattern, page, re.IGNORECASE):\n",
    "                matched_pages.append(n)\n",
    "        return matched_pages\n",
    "    return []\n",
    "\n",
    "def clean_text(text):\n",
    "    # 匹配从 \"Figures\\s{1}\\d\" 开始到换行符之间的所有内容\n",
    "    pattern = r'(Figures\\s{1}\\d.*?)\\n'\n",
    "    cleaned_text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    return cleaned_text\n",
    "\n",
    "# 示例使用\n",
    "text = \"\"\"\n",
    "Some text here.\n",
    "Figures 1 This is a figure description.\n",
    "More text here.\n",
    "\"\"\"\n",
    "\n",
    "cleaned_text = clean_text(text)\n",
    "print(cleaned_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
