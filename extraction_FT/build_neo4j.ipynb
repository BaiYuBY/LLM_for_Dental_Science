{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\Conda\\envs\\basic3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import neo4j\n",
    "import jsonlines\n",
    "import csv\n",
    "import pandas as pd\n",
    "from py2neo import Graph, Node, Relationship\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import os\n",
    "os.chdir(\"A:\\Desktop\\COMP7600\\dataset_label_fine-tune\")\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import faiss\n",
    "from transformers import ( \n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"bolt://192.168.0.117:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"Tzmt541881\"\n",
    "\n",
    "graph = Graph(uri, auth=(username, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    data = []\n",
    "    for file in glob.glob(os.path.join(file_path, \"*.jsonl\")):\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            for item in jsonlines.Reader(f):\n",
    "                data.append(item)\n",
    "    return data\n",
    "\n",
    "def filter_data(data):\n",
    "    entity_dict = ['frequency', 'examination', 'treatment', 'causes', 'materials', 'population', 'symptom', 'usage', 'clinical manifestations', \\\n",
    "        'medication information', 'dosage', 'instrument', 'oral part', 'description', 'prevention', 'disease', 'medicine']\n",
    "    title_list = ['relation_type', 'Entity1_type', 'Entity2_type', 'Entity1_name', 'Entity2_name']\n",
    "    relation_dict = {\n",
    "        'medicine_contraindicates_population': ['medicine', 'population'], \n",
    "        'treatment_has_description': ['treatment', 'description'], \n",
    "        'is': [], \n",
    "        'medicine_contraindication_disease': ['medicine', 'disease'], \n",
    "        'disease_caused_disease': ['disease', 'disease'], \n",
    "        'symptom_has_description': ['symptom', 'description'], \n",
    "        'medicine_side-effect_symptom': ['medicine','symptom'], \n",
    "        'disease_usually happens_population': ['disease', 'population'], \n",
    "        'disease_has_clinical manifestations': [], \n",
    "        'clinical manifestations_happens at_oral part': ['clinical manifestations', 'oral part'], \n",
    "        'causes_disease': ['causes', 'disease'], \n",
    "        'symptom_oral part': ['symptom', 'oral part'], \n",
    "        'clinical manifestations_use_treatment': ['clinical manifestations', 'treatment'], \n",
    "        'examination_has_description': ['examination', 'description'], \n",
    "        'symptom_cause_symptom': ['symptom', 'symptom'], \n",
    "        'medicine_medication-information': ['medicine','medication-information'], \n",
    "        'disease_has_description': ['disease', 'description'], \n",
    "        'oral part_discription': ['oral part', 'discription'],  \n",
    "        'examination_at_oral part': ['examination', 'oral part'], \n",
    "        'medicine_side-effect_disease': ['medicine', 'disease'], \n",
    "        'disease_symptom': ['disease', 'symptom'], \n",
    "        'disease_oral part': ['disease', 'oral part'], \n",
    "        'disease_examination': ['disease', 'examination'], \n",
    "        'prevent': [], \n",
    "        'disease_treatment': ['disease', 'treatment'], \n",
    "        'medicine_has_description': ['medicine', 'description'], \n",
    "        'disease_lack response_examination': ['disease', 'examination'], \n",
    "        'treatment_frequency': ['treatment', 'frequency'], \n",
    "        'medicine_reduce_symptom': ['medicine','symptom'], \n",
    "        'materials_use_as_medicine': ['materials', 'medicine'], \n",
    "        'instrument_discription': ['instrument', 'discription'], \n",
    "        'parent-child': [], \n",
    "        'medicine_treats_disease': ['medicine', 'disease'], \n",
    "        'clinical manifestations_from_examination': ['clinical manifestations', 'examination'], \n",
    "        'materials_has_description': ['materials', 'description']\n",
    "        }\n",
    "    check_list = ['medicine_medication-information', 'population_use_as_alternative_medicine', 'medication-information_frequency']\n",
    "    title_list = ['relation_type', 'Entity1_type', 'Entity1_name', 'Entity2_type', 'Entity2_name']\n",
    "    basic_dict = []\n",
    "    for item in data:\n",
    "        triples = item['response']\n",
    "        for triple in triples: \n",
    "            # print(item['id'],triple)\n",
    "            if any(item not in title_list for item in triple.keys()) or any(item not in triple.keys() for item in title_list)\\\n",
    "                or not all(item in triple.keys() for item in title_list) or any(item == None for item in triple.values()):\n",
    "                continue\n",
    "\n",
    "            triple['relation_type'] = triple['relation_type'].replace('medication-information_frequency', 'medication information_frequency')\\\n",
    "                            .replace('Causes_disease', 'causes_disease').replace('symptom_oralpart', 'symptom_oral part')\\\n",
    "                                .replace('disease_oral-part', 'disease_oral part').replace('materials_use_as_medicine', 'materials_use as_medicine').lower()\n",
    "                \n",
    "            triple['Entity1_type'] = str(triple['Entity1_type']).replace('Causes', 'causes').lower()\n",
    "            triple['Entity2_type'] = str(triple['Entity2_type']).replace('Causes', 'causes').lower()\n",
    "            triple['Entity1_name'] = str(triple['Entity1_name']).lower()\n",
    "            triple['Entity2_name'] = str(triple['Entity2_name']).lower()\n",
    "            \n",
    "            # if not all(item in triple.keys() for item in title_list):\n",
    "                # continue\n",
    "\n",
    "            # if any(item == None for item in triple.values()):\n",
    "                # continue\n",
    "\n",
    "            if triple['relation_type'] not in check_list and triple['relation_type'] in relation_dict.keys()\\\n",
    "                and triple['Entity1_name'] not in entity_dict and triple['Entity2_name'] not in entity_dict\\\n",
    "                    and triple['Entity1_type'] in entity_dict and triple['Entity2_type'] in entity_dict:\n",
    "                if relation_dict[triple['relation_type']] == []:\n",
    "                    basic_dict.append({\n",
    "                        'source': item['id'],\n",
    "                        'data': triple\n",
    "                    })\n",
    "                else:\n",
    "                    if (triple['Entity2_type'] == relation_dict[triple['relation_type']][0] \\\n",
    "                        and triple['Entity1_type'] == relation_dict[triple['relation_type']][1])\\\n",
    "                         or (triple['Entity1_type'] == relation_dict[triple['relation_type']][0] \\\n",
    "                            and triple['Entity2_type'] == relation_dict[triple['relation_type']][1]):\n",
    "                        basic_dict.append({\n",
    "                            'source': item['id'],\n",
    "                            'data': triple\n",
    "                        })\n",
    "    return basic_dict\n",
    "\n",
    "def clean_string(s):\n",
    "    return re.sub(r'\\s+', ' ', s.strip())\n",
    "\n",
    "def get_or_create_entity(entity_id, entity_type, entity_name, entity_source):\n",
    "    if entity_type not in entities_dict:\n",
    "        entities_dict[entity_type] = {}\n",
    "    \n",
    "    if entity_name not in entities_dict[entity_type]:\n",
    "        # 创建实体\n",
    "        node = Node(entity_type, id=entity_id, name=entity_name, source=entity_source, )\n",
    "        graph.create(node)\n",
    "        entities_dict[entity_type][entity_name] = node\n",
    "        logging.info(f'Entity node created: {entity_type}_{entity_name}.')\n",
    "        return node\n",
    "    else:\n",
    "        # 获取已存在的实体节点\n",
    "        node = entities_dict[entity_type][entity_name]\n",
    "        logging.info(f'Entity already exists: {entity_type}_{entity_name}!')\n",
    "\n",
    "        # 追加新的 source 信息\n",
    "        if 'source' in node:\n",
    "            existing_source = node['source']\n",
    "            if isinstance(existing_source, list):\n",
    "                existing_source.append(entity_source)\n",
    "            else:\n",
    "                node['source'] = f\"{existing_source}; {entity_source}\"\n",
    "        else:\n",
    "            node['source'] = entity_source\n",
    "\n",
    "        # 保存更新后的节点\n",
    "        graph.push(node)\n",
    "        logging.info(f'Entity source updated for {entity_type}_{entity_name}.')\n",
    "        \n",
    "        return node\n",
    "\n",
    "def upload_neo4j(df):\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        x_node = get_or_create_entity(row['x_id'], row['subject_type'], row['subject'].replace('’', \"'\"), row['source'])\n",
    "        y_node = get_or_create_entity(row['y_id'], row['object_type'], row['object'].replace('’', \"'\"), row['source'])\n",
    "\n",
    "        relationship = Relationship(x_node, row['relation'], y_node)\n",
    "        graph.create(relationship)\n",
    "        logging.info(f\"Relationship created: {row['relation']}.\")\n",
    "\n",
    "def id_check(id_table, name, i):\n",
    "    if name not in id_table.keys():\n",
    "        id_table[name] = i\n",
    "        return id_table, i + 1\n",
    "    else:\n",
    "        return id_table, i\n",
    "\n",
    "def convert_to_csv(kg_path, basic_dict):\n",
    "    with open(kg_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['x_id', 'subject', 'subject_type', 'relation', 'y_id', 'object', 'object_type', 'source']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        id_table = {}\n",
    "        writer.writeheader()\n",
    "        i = 1\n",
    "        for triplet in basic_dict:\n",
    "            source = triplet['source']\n",
    "            triplet = triplet['data']\n",
    "            \n",
    "            id_table, i = id_check(id_table, triplet['Entity1_name'], i)\n",
    "            id_table, i = id_check(id_table, triplet['Entity2_name'], i)\n",
    "\n",
    "            # print(id_table)\n",
    "            writer.writerow({\n",
    "                'x_id': id_table[triplet['Entity1_name']],\n",
    "                'subject': triplet['Entity1_name'],\n",
    "                'subject_type': triplet['Entity1_type'],\n",
    "                'relation': triplet['relation_type'],\n",
    "                'y_id': id_table[triplet['Entity2_name']],\n",
    "                'object': triplet['Entity2_name'],\n",
    "                'object_type': triplet['Entity2_type'],\n",
    "                'source': source\n",
    "            })\n",
    "        return id_table\n",
    "        \n",
    "def load_model_and_tokenizer():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        './models/Llama3-Med42-8B',\n",
    "        device_map={\"\":0},\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./models/Llama3-Med42-8B')\n",
    "    # tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './extraction_R1'\n",
    "kg_path = './kg1.csv'\n",
    "id_table = convert_to_csv(kg_path, filter_data(load_data(file_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " distinct_entities_count \n",
      "-------------------------\n",
      "                    9464 \n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"\"\"\n",
    "MATCH (n)\n",
    "RETURN COUNT(DISTINCT n) AS distinct_entities_count\"\"\"\n",
    "result = graph.run(query)\n",
    "print(result)\n",
    "# with open('test.txt', 'w') as f:\n",
    "#     f.write(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查询数据\n",
    "query = \"\"\"\n",
    "MATCH (n)-[r]->(m)\n",
    "RETURN n, r, m\n",
    "LIMIT 50\n",
    "\"\"\"\n",
    "data = graph.run(query).data()\n",
    "\n",
    "# 创建NetworkX图\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# 添加节点和边\n",
    "for record in data:\n",
    "    n = record['n']\n",
    "    m = record['m']\n",
    "    r = record['r']\n",
    "    G.add_node(n['name'], label=n['name'])\n",
    "    G.add_node(m['name'], label=m['name'])\n",
    "    G.add_edge(n['name'], m['name'], label=r['name'])\n",
    "\n",
    "# 绘制图形\n",
    "pos = nx.spring_layout(G)  # 使用Spring布局\n",
    "labels = nx.get_edge_attributes(G, 'label')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrows=True)\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=labels, font_color='red')\n",
    "\n",
    "plt.title(\"Neo4j Graph Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有数据已清除\n"
     ]
    }
   ],
   "source": [
    "# 清除数据库中的所有数据\n",
    "graph.run(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "print(\"所有数据已清除\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14910/14910 [21:46<00:00, 11.41it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "csv_file = './kg1.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.applymap(lambda x: clean_string(x) if isinstance(x, str) else x)\n",
    "\n",
    "entities_dict = {}\n",
    "logging.basicConfig(filename='neo4j_import.log', level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "upload_neo4j(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.97s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer =load_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings = np.load('./combined_entity_embeddings.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8115\n"
     ]
    }
   ],
   "source": [
    "print(len(entities_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_list = []\n",
    "entities_list = []\n",
    "for item, id in id_table.items():\n",
    "    ids_list.append(id)\n",
    "    entities_list.append(item)\n",
    "\n",
    "entities_ids = np.array(ids_list, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[574, 575]\n",
      "['bony defect', 'deep pocketing']\n"
     ]
    }
   ],
   "source": [
    "print(ids_list[573:575])\n",
    "print(entities_list[573:575])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_table_list = [{'name': entities_list[i], 'id': ids_list[i]} for i in range(len(ids_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_table_list = [{'name': entities_list[i], 'id': ids_list[i]} for i in range(len(ids_list))]\n",
    "df = pd.DataFrame(id_table_list)\n",
    "df.to_csv('table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_embedding(query):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    outputs = model(**inputs)\n",
    "    cls_embedding = outputs.hidden_states[-1][:, 0, :].numpy()\n",
    "    return cls_embedding\n",
    "\n",
    "def search_similar_entities(query, index, entities, top_k=5):\n",
    "    query_embedding = query_to_embedding(query)\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "    similar_entities = [entities[i] for i in I[0]]\n",
    "    return similar_entities\n",
    "\n",
    "dimension = loaded_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index_with_ids = faiss.IndexIDMap(index)\n",
    "index_with_ids.add_with_ids(loaded_embeddings, entities_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6865"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.549   0.2358 -0.902  ... -1.55    0.322   2.422 ]\n",
      " [ 2.549   0.234  -0.9014 ... -1.552   0.3215  2.422 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question_embedding = np.load('./quesitons (1).npy')\n",
    "temp_1 = np.load('./combined_entity_embeddings.npy')\n",
    "temp_2 = temp_1[5217]\n",
    "print(question_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "l2_distance = np.linalg.norm(question_embedding[0] - temp_1)\n",
    "print(l2_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 2.5488281 ,  0.23583984, -0.90185547, ..., -1.5498047 ,\n",
      "         0.32202148,  2.421875  ]], dtype=float32), array([[ 2.5488281 ,  0.23400879, -0.9013672 , ..., -1.5517578 ,\n",
      "         0.3215332 ,  2.421875  ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "query_vector = []\n",
    "for question in question_embedding:\n",
    "    query_vector.append(np.array([question], dtype=np.float32))\n",
    "print(query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances: [[0.02630468 0.02630468 0.02630468 0.02630468 0.02648782]]\n",
      "Resulting IDs: [[1528 1664 3859 6278   13]]\n"
     ]
    }
   ],
   "source": [
    "distances, result_ids = index_with_ids.search(query_vector[0], k=5)\n",
    "print(\"Distances:\", distances)\n",
    "print(\"Resulting IDs:\", result_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "di-calcium silicate\n",
      "enamel lost rapidly due to defective edj brown; opalescent dentine colour; prone to fracture/wear short roots, bulbous crowns; pulps obliterated\n",
      "neurosensory impairment\n",
      "vital pulp therapy\n",
      "external cervical resorption\n"
     ]
    }
   ],
   "source": [
    "for i in result_ids[0]:\n",
    "    print(entities_list[int(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]b:\\Conda\\envs\\basic3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'index_with_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m     19\u001b[0m     query_vector\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray([question], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# l2_distance = np.linalg.norm(question_embedding[0] - question_embedding[1])\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# print(l2_distance)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m distances, result_ids \u001b[38;5;241m=\u001b[39m \u001b[43mindex_with_ids\u001b[49m\u001b[38;5;241m.\u001b[39msearch(query_vector[\u001b[38;5;241m0\u001b[39m], k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistances:\u001b[39m\u001b[38;5;124m\"\u001b[39m, distances)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResulting IDs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result_ids)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'index_with_ids' is not defined"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedder_model = SentenceTransformer(\"A:\\Desktop\\COMP7600\\dataset_label_fine-tune/models/pubmedbert-base-embeddings\")\n",
    "\n",
    "def get_entity_embeddings(entities):\n",
    "    embeddings = []\n",
    "    for entity in tqdm(entities):\n",
    "        output_embedding = embedder_model.encode(entity)\n",
    "        embeddings.append(output_embedding)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "question_list = ['A biopsy specimen of the lower lip salivary glands showed replacement of parenchymal tissue by lymphocytes. The patient also had xerostomia and eratoconjunctivitis sicca. These findings are indicative of which disease?']\n",
    "# question_list = ['xerostomia', 'keratoconjunctivitis sicca']\n",
    "\n",
    "question_embedding = get_entity_embeddings(question_list)\n",
    "# print(question_embedding)\n",
    "\n",
    "query_vector = []\n",
    "for question in question_embedding:\n",
    "    query_vector.append(np.array([question], dtype=np.float32))\n",
    "\n",
    "# l2_distance = np.linalg.norm(question_embedding[0] - question_embedding[1])\n",
    "# print(l2_distance)\n",
    "\n",
    "distances, result_ids = index_with_ids.search(query_vector[0], k=5)\n",
    "print(\"Distances:\", distances)\n",
    "print(\"Resulting IDs:\", result_ids)\n",
    "\n",
    "for i in result_ids[0]:\n",
    "    print(entities_list[int(i-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "np.save('question.npy', question_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedder_model = SentenceTransformer(\"./pubmedbert-base-embeddings\")\n",
    "\n",
    "def get_entity_embeddings(entities):\n",
    "    embeddings = []\n",
    "    for entity in tqdm(entities):\n",
    "        output_embedding = embedder_model.encode(entity)\n",
    "        embeddings.append(output_embedding)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "question_list = ['xerostomia', 'keratoconjunctivitis sicca']\n",
    "\n",
    "question_embedding = get_entity_embeddings(question_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "cosine_sim = 1 - cosine(test[0], test[1])\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    \n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    cosine_sim = dot_product / (norm_vec1 * norm_vec2)\n",
    "    \n",
    "    return cosine_sim\n",
    "\n",
    "l2_distance = cosine_similarity(test[0], test[1])\n",
    "print(cosine_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
