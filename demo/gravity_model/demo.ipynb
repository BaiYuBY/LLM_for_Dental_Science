{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./kg1.csv')\n",
    "\n",
    "# 提取三元组\n",
    "triples = df[['subject', 'relation', 'object']]\n",
    "\n",
    "# 提取实体及其类型\n",
    "entity_types = pd.concat([\n",
    "    df[['subject', 'subject_type']].rename(columns={'subject': 'entity', 'subject_type': 'type'}),\n",
    "    df[['object', 'object_type']].rename(columns={'object': 'entity', 'object_type': 'type'})\n",
    "]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7936\n"
     ]
    }
   ],
   "source": [
    "print(len(entity_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples[:2000].to_csv(\"triples_part1.csv\", index=False)\n",
    "# entity_types.to_csv(\"entity_types.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease\n",
      "                 subject                           relation  \\\n",
      "0       axial herniation             disease_caused_disease   \n",
      "1     lateral herniation             disease_caused_disease   \n",
      "2            hemiparesis            symptom_has_description   \n",
      "3       axial herniation                disease_examination   \n",
      "4     lateral herniation                disease_examination   \n",
      "...                  ...                                ...   \n",
      "3172           composite  medicine_contraindication_disease   \n",
      "3173           composite            medicine_treats_disease   \n",
      "3174               macro           examination_at_oral part   \n",
      "3175               macro           examination_at_oral part   \n",
      "3176             packing           examination_at_oral part   \n",
      "\n",
      "                                                 object  \n",
      "0                               generalized brain edema  \n",
      "1                                unilateral mass effect  \n",
      "2     pressing the opposite cerebral peduncle agains...  \n",
      "3                                  brainstem infarction  \n",
      "4      compression of the ipsilateral cerebral peduncle  \n",
      "...                                                 ...  \n",
      "3172                                            dentine  \n",
      "3173                                            dentine  \n",
      "3174                                               gold  \n",
      "3175                                          porcelain  \n",
      "3176                                        nayyar core  \n",
      "\n",
      "[3177 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "file_path = './export4000.csv'\n",
    "entity_pattern = r'name: \"(.*?)\".*?id: (\\d+)'  # 提取实体名称和ID\n",
    "relation_pattern = r'\\[:(.*?)\\]'              # 提取关系\n",
    "out_data = []\n",
    "entity_type_counter = {}\n",
    "\n",
    "# 统计每个实体的类型出现次数\n",
    "with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # 跳过标题行\n",
    "    for data in reader:\n",
    "        # 提取实体1和类型\n",
    "        entity1_match = re.search(entity_pattern, data[0])\n",
    "        if entity1_match:\n",
    "            entity1_name = entity1_match.group(1)\n",
    "            entity1_type = re.search(r'\\(:([a-zA-Z_]+)', data[0]).group(1)\n",
    "\n",
    "            # 统计类型出现次数\n",
    "            if entity1_name not in entity_type_counter:\n",
    "                entity_type_counter[entity1_name] = Counter()\n",
    "            entity_type_counter[entity1_name][entity1_type] += 1\n",
    "\n",
    "        # 提取实体2和类型\n",
    "        entity2_match = re.search(entity_pattern, data[2])\n",
    "        if entity2_match:\n",
    "            entity2_name = entity2_match.group(1)\n",
    "            entity2_type = re.search(r'\\(:([a-zA-Z_]+)', data[2]).group(1)\n",
    "\n",
    "            # 统计类型出现次数\n",
    "            if entity2_name not in entity_type_counter:\n",
    "                entity_type_counter[entity2_name] = Counter()\n",
    "            entity_type_counter[entity2_name][entity2_type] += 1\n",
    "\n",
    "# 找到每个实体出现次数最多的类型\n",
    "entity_most_common_type = {\n",
    "    entity: types.most_common(1)[0][0]\n",
    "    for entity, types in entity_type_counter.items()\n",
    "}\n",
    "\n",
    "print(entity_most_common_type['periodontal disease'])\n",
    "# 生成输出三元组，过滤掉非最多类型的实体\n",
    "with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # 跳过标题行\n",
    "    for data in reader:\n",
    "        entity1_match = re.search(entity_pattern, data[0])\n",
    "        entity2_match = re.search(entity_pattern, data[2])\n",
    "        relation_match = re.search(relation_pattern, data[1])\n",
    "\n",
    "        if entity1_match and entity2_match and relation_match:\n",
    "            entity1_name = entity1_match.group(1)\n",
    "            entity1_type = re.search(r'\\(:([a-zA-Z_]+)', data[0]).group(1)\n",
    "            entity2_name = entity2_match.group(1)\n",
    "            entity2_type = re.search(r'\\(:([a-zA-Z_]+)', data[2]).group(1)\n",
    "            relation = relation_match.group(1)\n",
    "\n",
    "            # 保留每个实体最多类型的记录\n",
    "            if (\n",
    "                entity1_type == entity_most_common_type[entity1_name] and\n",
    "                entity2_type == entity_most_common_type[entity2_name]\n",
    "            ):\n",
    "                out_data.append({\n",
    "                    'subject': entity1_name,\n",
    "                    'relation': relation,\n",
    "                    'object': entity2_name,\n",
    "                })\n",
    "\n",
    "# 转换为 DataFrame 并显示结果\n",
    "out_df = pd.DataFrame(out_data)\n",
    "print(out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_csv(\"triples_part5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6354\n"
     ]
    }
   ],
   "source": [
    "entity_type_pattern = r'\\(:([a-zA-Z_]+)'  # 提取实体类型\n",
    "name_pattern = r'name: \"(.*?)\"'           # 提取实体名字\n",
    "\n",
    "# 用于存储结果和类型统计\n",
    "results = []\n",
    "entity_type_counter = {}\n",
    "\n",
    "# 统计每个实体的类型出现次数\n",
    "with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # 跳过标题行\n",
    "    for line in reader:\n",
    "        # 提取第一个实体及类型\n",
    "        entity_type_match_1 = re.search(entity_type_pattern, line[0])\n",
    "        name_match_1 = re.search(name_pattern, line[0])\n",
    "        if entity_type_match_1 and name_match_1:\n",
    "            entity_type_1 = entity_type_match_1.group(1)\n",
    "            name_1 = name_match_1.group(1)\n",
    "            \n",
    "            # 更新类型统计\n",
    "            if name_1 not in entity_type_counter:\n",
    "                entity_type_counter[name_1] = Counter()\n",
    "            entity_type_counter[name_1][entity_type_1] += 1\n",
    "\n",
    "        # 提取第二个实体及类型\n",
    "        entity_type_match_2 = re.search(entity_type_pattern, line[2])\n",
    "        name_match_2 = re.search(name_pattern, line[2])\n",
    "        if entity_type_match_2 and name_match_2:\n",
    "            entity_type_2 = entity_type_match_2.group(1)\n",
    "            name_2 = name_match_2.group(1)\n",
    "\n",
    "            # 更新类型统计\n",
    "            if name_2 not in entity_type_counter:\n",
    "                entity_type_counter[name_2] = Counter()\n",
    "            entity_type_counter[name_2][entity_type_2] += 1\n",
    "\n",
    "# 找到每个实体的最多类型\n",
    "entity_most_common_type = {\n",
    "    entity: types.most_common(1)[0][0]\n",
    "    for entity, types in entity_type_counter.items()\n",
    "}\n",
    "\n",
    "# 筛选出符合最多类型的实体\n",
    "with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # 跳过标题行\n",
    "    for line in reader:\n",
    "        entity_type_match_1 = re.search(entity_type_pattern, line[0])\n",
    "        name_match_1 = re.search(name_pattern, line[0])\n",
    "        entity_type_match_2 = re.search(entity_type_pattern, line[2])\n",
    "        name_match_2 = re.search(name_pattern, line[2])\n",
    "\n",
    "        if entity_type_match_1 and name_match_1 and entity_type_match_2 and name_match_2:\n",
    "            entity_type_1 = entity_type_match_1.group(1)\n",
    "            name_1 = name_match_1.group(1)\n",
    "            entity_type_2 = entity_type_match_2.group(1)\n",
    "            name_2 = name_match_2.group(1)\n",
    "\n",
    "            # 仅保留每个实体为其最多类型的记录\n",
    "            if (entity_type_1 == entity_most_common_type[name_1] and\n",
    "                entity_type_2 == entity_most_common_type[name_2]):\n",
    "                results.append({\n",
    "                    \"subject_type\": entity_type_1,\n",
    "                    \"subject_name\": name_1,\n",
    "                })\n",
    "                results.append({\n",
    "                    \"subject_type\": entity_type_2,\n",
    "                    \"subject_name\": name_2,\n",
    "                })\n",
    "\n",
    "\n",
    "# 转换为 DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# 显示 DataFrame 长度和内容\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2850\n"
     ]
    }
   ],
   "source": [
    "unique_df = df.drop_duplicates()\n",
    "print(len(unique_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique_df.to_csv(\"unique_entities_part5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
