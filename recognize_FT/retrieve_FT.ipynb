{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os \n",
    "os.chdir(r'A:\\Desktop\\COMP7600\\dataset_label_fine-tune\\retrieve_part')\n",
    "import jsonlines\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with open('val.txt', 'r') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "def find_sentences(text):\n",
    "    pattern = re.compile(r'(?m)^(?:\\d+\\.|# \\d+\\.)(.*)')\n",
    "    matches = pattern.findall(text)\n",
    "    return [match.strip() for match in matches]\n",
    "\n",
    "def storage(text):\n",
    "    with open('retrieve_data.txt', 'a') as file:\n",
    "        for item in text:\n",
    "            if len(item.split(' ')) > 5:\n",
    "                file.write(item + '\\n')\n",
    "\n",
    "def convert_formats(data):\n",
    "    converted_data = []\n",
    "    for doc in data:\n",
    "        if doc['entities'] == []:\n",
    "            continue\n",
    "        temp = {}\n",
    "        temp['id'] = doc['id']\n",
    "        temp['text'] = doc['text']\n",
    "        temp_total = []\n",
    "        temp_entity = []\n",
    "\n",
    "        for e in doc['entities']:\n",
    "            start_point = e[0]\n",
    "            end_point = e[1]\n",
    "            temp_entity.append(temp['text'][start_point:end_point])\n",
    "        temp['entity_list'] = temp_entity\n",
    "        converted_data.append(temp)\n",
    "    return converted_data\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        './models/Llama3-Med42-8B',\n",
    "        device_map={\"\":0},\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./models/Llama3-Med42-8B')\n",
    "    # tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer\n",
    "\n",
    "def prompt_generation(data):\n",
    "    dataset = []\n",
    "\n",
    "    INSTRUCTION = 'Identify the important entities in the question that you need further extral knowledge to answer.'\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "    The following is an unstructured question, please extract the important entities in the text, and the output should follow the following format: [entity1, entity2, entity3]. No more note or explain is needed, only output a list.\n",
    "    This extracted entities are used for searching further help and knowledge in an extal database. Thus, only find the entities that you are not sure or not familiar with. If you cannot find any entities that satisfy the above requirements, please output: []. No more note or explan is needed.\n",
    "    \"\"\".strip().strip('\\n')\n",
    "    for i in range(len(data)):\n",
    "        dataset.append({\n",
    "            'input': data[i]['text'],\n",
    "            'output': str(data[i]['entity_list']).replace(\"'\", ''),\n",
    "            'prompt': '### Instruction: \\n' + INSTRUCTION + '\\n\\n### System Prompt: \\n' +\\\n",
    "             SYSTEM_PROMPT + '\\n\\n### Input: \\n' + data[i]['text'] + '\\n\\n### Response:'\n",
    "        })\n",
    "    with open('test.txt', 'w') as f:\n",
    "        f.write(str(dataset))\n",
    "    return Dataset.from_pandas(pd.DataFrame(data=dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file = './jiarenmen.jsonl'\n",
    "all_data = []\n",
    "with open(all_file, 'r', encoding='utf-8') as file:\n",
    "    for item in jsonlines.Reader(file):\n",
    "        all_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DatasetDict({\n",
    "    'tarin': prompt_generation(convert_formats(all_data))\n",
    "}).shuffle(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer()\n",
    "model.config.quantization_config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"],\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results\"\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 2\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 1\n",
    "num_train_epochs = 4\n",
    "logging_steps = 1\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 20\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
